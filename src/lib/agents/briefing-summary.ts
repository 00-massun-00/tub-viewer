// â”€â”€ AI Briefing Summary Agent â”€â”€
// Generates an executive summary of search/browse results using LLM.
// Falls back to a rule-based summary when LLM is unavailable.

import { UpdateItem, SupportedLocale } from "../types";
import { getConfig, isLlmAvailable } from "../config";
import { createLogger } from "../logger";
import OpenAI from "openai";

const logger = createLogger("BriefingSummary");

export interface BriefingSummaryResult {
  summary: string;
  method: "llm" | "rule-based";
  durationMs: number;
}

const SUMMARY_PROMPT_TEMPLATES: Record<string, string> = {
  ja: `ã‚ãªãŸã¯Microsoftè£½å“ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæƒ…å ±ã‚’ã‚‚ã¨ã«ã€ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆå‘ã‘ã®3è¡Œä»¥å†…ã®ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚`,
  en: `You are a Microsoft technology advisor. Based on the following update information, generate a concise executive summary (max 3 sentences) for management. Focus on the most critical items.`,
  ko: `ë‹¹ì‹ ì€ Microsoft ì œí’ˆ ê¸°ìˆ  ê³ ë¬¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ì˜ì§„ì„ ìœ„í•œ 3ì¤„ ì´ë‚´ì˜ ìš”ì•½ì„ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ì„¸ìš”.`,
  zh: `æ‚¨æ˜¯Microsoftäº§å“æŠ€æœ¯é¡¾é—®ã€‚æ ¹æ®ä»¥ä¸‹æ›´æ–°ä¿¡æ¯ï¼Œç”¨ä¸­æ–‡ç”Ÿæˆé¢å‘ç®¡ç†å±‚çš„3å¥ä»¥å†…æ‘˜è¦ã€‚`,
  es: `Eres un asesor tÃ©cnico de Microsoft. Genera un resumen ejecutivo de mÃ¡ximo 3 oraciones basado en la siguiente informaciÃ³n.`,
  fr: `Vous Ãªtes un conseiller technique Microsoft. GÃ©nÃ©rez un rÃ©sumÃ© exÃ©cutif de 3 phrases maximum.`,
  de: `Sie sind ein Microsoft-Technologieberater. Erstellen Sie eine Zusammenfassung in maximal 3 SÃ¤tzen.`,
  pt: `VocÃª Ã© um consultor tÃ©cnico da Microsoft. Gere um resumo executivo de no mÃ¡ximo 3 frases.`,
};

/**
 * Generate an AI-powered executive summary of update results.
 */
export async function generateBriefingSummary(
  updates: UpdateItem[],
  locale: SupportedLocale,
  query?: string,
): Promise<BriefingSummaryResult> {
  const startTime = Date.now();

  if (updates.length === 0) {
    return {
      summary: "",
      method: "rule-based",
      durationMs: Date.now() - startTime,
    };
  }

  // Try LLM summary
  if (isLlmAvailable()) {
    try {
      const config = getConfig();
      const client = new OpenAI({ apiKey: config.openaiApiKey });
      const systemPrompt = SUMMARY_PROMPT_TEMPLATES[locale] || SUMMARY_PROMPT_TEMPLATES.en;

      const updateSummary = updates.slice(0, 10).map((u, i) => {
        const severity = u.severity === "breaking" ? "ğŸ”´ CRITICAL" : u.severity === "new-feature" ? "ğŸŸ¡ NEW" : "ğŸŸ¢ INFO";
        return `${i + 1}. [${severity}] ${u.titleEn || u.title}: ${u.summaryEn || u.summary}`;
      }).join("\n");

      const userMessage = query
        ? `Query: "${query}"\n\nUpdates (${updates.length} total):\n${updateSummary}`
        : `Updates (${updates.length} total):\n${updateSummary}`;

      const response = await client.chat.completions.create({
        model: config.openaiModel,
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: userMessage },
        ],
        max_tokens: 200,
        temperature: 0.3,
      });

      const summary = response.choices[0]?.message?.content?.trim() || "";
      const durationMs = Date.now() - startTime;
      logger.info("LLM briefing summary generated", { durationMs, length: summary.length });

      return { summary, method: "llm", durationMs };
    } catch (error) {
      logger.warn("LLM summary failed, falling back to rule-based", {
        error: error instanceof Error ? error.message : String(error),
      });
    }
  }

  // Rule-based fallback summary
  const summary = generateRuleBasedSummary(updates, locale);
  return {
    summary,
    method: "rule-based",
    durationMs: Date.now() - startTime,
  };
}

function generateRuleBasedSummary(updates: UpdateItem[], locale: SupportedLocale): string {
  const breaking = updates.filter((u) => u.severity === "breaking");
  const newFeatures = updates.filter((u) => u.severity === "new-feature");
  const improvements = updates.filter((u) => u.severity === "improvement");

  if (locale === "ja") {
    const parts: string[] = [];
    if (breaking.length > 0) {
      parts.push(`ğŸ”´ ${breaking.length}ä»¶ã®è¦å¯¾å¿œé …ç›®ãŒã‚ã‚Šã¾ã™ã€‚${breaking[0].title}ãªã©ã€æ—©æ€¥ãªç¢ºèªãŒå¿…è¦ã§ã™ã€‚`);
    }
    if (newFeatures.length > 0) {
      parts.push(`ğŸŸ¡ ${newFeatures.length}ä»¶ã®æ–°æ©Ÿèƒ½ãŒè¿½åŠ ã•ã‚Œã¦ã„ã¾ã™ã€‚`);
    }
    if (improvements.length > 0) {
      parts.push(`ğŸŸ¢ ${improvements.length}ä»¶ã®æ”¹å–„ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚`);
    }
    return parts.join(" ") || `${updates.length}ä»¶ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚`;
  }

  const parts: string[] = [];
  if (breaking.length > 0) {
    parts.push(`ğŸ”´ ${breaking.length} breaking change(s) require immediate attention, including "${breaking[0].titleEn || breaking[0].title}".`);
  }
  if (newFeatures.length > 0) {
    parts.push(`ğŸŸ¡ ${newFeatures.length} new feature(s) available for review.`);
  }
  if (improvements.length > 0) {
    parts.push(`ğŸŸ¢ ${improvements.length} improvement(s) included.`);
  }
  return parts.join(" ") || `${updates.length} update(s) found.`;
}
